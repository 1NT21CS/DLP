{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnTAUalnQgj-",
        "outputId": "1a321673-0148-40a4-8609-8d904d9eaafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "\u001b[1m2110848/2110848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
            "\u001b[1m550378/550378\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.3061 - loss: 3.3249 - val_accuracy: 0.6320 - val_loss: 1.9802\n",
            "Epoch 2/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.6600 - loss: 1.7617 - val_accuracy: 0.6810 - val_loss: 1.4250\n",
            "Epoch 3/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.7318 - loss: 1.2649 - val_accuracy: 0.7370 - val_loss: 1.2269\n",
            "Epoch 4/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.7875 - loss: 1.0222 - val_accuracy: 0.7630 - val_loss: 1.1146\n",
            "Epoch 5/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.8246 - loss: 0.8218 - val_accuracy: 0.7790 - val_loss: 1.0608\n",
            "Epoch 6/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8480 - loss: 0.7151 - val_accuracy: 0.7920 - val_loss: 0.9702\n",
            "Epoch 7/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8762 - loss: 0.5953 - val_accuracy: 0.8040 - val_loss: 0.9279\n",
            "Epoch 8/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9061 - loss: 0.4863 - val_accuracy: 0.8060 - val_loss: 0.9124\n",
            "Epoch 9/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.9129 - loss: 0.4193 - val_accuracy: 0.7980 - val_loss: 0.9158\n",
            "Epoch 10/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.9250 - loss: 0.3629 - val_accuracy: 0.8050 - val_loss: 0.9285\n",
            "Epoch 11/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9334 - loss: 0.3083 - val_accuracy: 0.8130 - val_loss: 0.8704\n",
            "Epoch 12/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9426 - loss: 0.2644 - val_accuracy: 0.8190 - val_loss: 0.8593\n",
            "Epoch 13/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9509 - loss: 0.2253 - val_accuracy: 0.8090 - val_loss: 0.8812\n",
            "Epoch 14/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9568 - loss: 0.1983 - val_accuracy: 0.8100 - val_loss: 0.9204\n",
            "Epoch 15/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9531 - loss: 0.1847 - val_accuracy: 0.7920 - val_loss: 0.9407\n",
            "Epoch 16/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9546 - loss: 0.1771 - val_accuracy: 0.8190 - val_loss: 0.9166\n",
            "Epoch 17/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9575 - loss: 0.1558 - val_accuracy: 0.8070 - val_loss: 0.9445\n",
            "Epoch 18/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9604 - loss: 0.1344 - val_accuracy: 0.8080 - val_loss: 0.9304\n",
            "Epoch 19/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.9595 - loss: 0.1390 - val_accuracy: 0.8020 - val_loss: 0.9820\n",
            "Epoch 20/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.9565 - loss: 0.1340 - val_accuracy: 0.8150 - val_loss: 0.9551\n",
            "Epoch 21/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9600 - loss: 0.1213 - val_accuracy: 0.8090 - val_loss: 0.9698\n",
            "Epoch 22/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9639 - loss: 0.1085 - val_accuracy: 0.8100 - val_loss: 0.9658\n",
            "Epoch 23/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9587 - loss: 0.1214 - val_accuracy: 0.8130 - val_loss: 0.9819\n",
            "Epoch 24/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9617 - loss: 0.1106 - val_accuracy: 0.8050 - val_loss: 1.0118\n",
            "Epoch 25/25\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9630 - loss: 0.1041 - val_accuracy: 0.8100 - val_loss: 1.0140\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8036 - loss: 1.0611\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import tensorflow as tf  # import tensor flow\n",
        "import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import reuters\n",
        "\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
        "\n",
        "\n",
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "\n",
        "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "def to_one_hot(labels, dimension=46):\n",
        "    results = np.zeros((len(labels), dimension))\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "\n",
        "\n",
        "one_hot_train_labels = to_one_hot(train_labels)\n",
        "one_hot_test_labels = to_one_hot(test_labels)\n",
        "\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)\n",
        "\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(46, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "x_val = x_train[:1000]\n",
        "partial_x_train = x_train[1000:]\n",
        "y_val = one_hot_train_labels[:1000]\n",
        "partial_y_train = one_hot_train_labels[1000:]\n",
        "\n",
        "\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=25,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))\n",
        "\n",
        "\n",
        "results = model.evaluate(x_test, one_hot_test_labels)\n"
      ]
    }
  ]
}